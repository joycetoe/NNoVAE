{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activate conda env keras.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras, numpy as np, os, pandas as pd, time, random\n",
    "import subprocess, re, sys, os\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers.core import Lambda\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "import tensorflow\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set defaults \n",
    "seed = 374173\n",
    "latent_dim = 3\n",
    "weight = 1000\n",
    "k = 8\n",
    "train_prop = 0.9\n",
    "depth = 6\n",
    "width = 128\n",
    "patience = 100\n",
    "batch_size=32\n",
    "prediction_freq=2\n",
    "max_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = 'training/'\n",
    "KMER = '../../data/kmers/GCrefv1_kmers.npy'\n",
    "META = '../../data/metadata/GCrefv1.tsv'\n",
    "NAMES = '../../data/GCrefv1_samples.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(KMER)\n",
    "names = pd.read_csv(NAMES)['sampleID']\n",
    "meta = pd.read_csv(META, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(782, 65536) (782,) (782, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, names.shape, meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tensorflow.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training and test data\n",
    "ninds = data.shape[0]\n",
    "train = np.random.choice(range(ninds), int(train_prop*ninds), replace=False)\n",
    "test = np.array([x for x in range(ninds) if x not in train])\n",
    "traindata=data[train,:]\n",
    "testdata=data[test,:]\n",
    "trainsamples=names.loc[train]\n",
    "testsamples=names.loc[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\n",
    "                             mean=0., stddev=1.,seed=seed)\n",
    "    return z_mean + K.exp(z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "input_seq = keras.Input(shape=(traindata.shape[1],))\n",
    "x=layers.Dense(width,activation=\"elu\")(input_seq)\n",
    "for i in range(depth-1):\n",
    "    x=layers.Dense(width,activation=\"elu\")(x)\n",
    "z_mean=layers.Dense(latent_dim)(x)\n",
    "z_log_var=layers.Dense(latent_dim)(x)\n",
    "z = layers.Lambda(sampling,output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "encoder=Model(input_seq,[z_mean,z_log_var,z],name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder\n",
    "decoder_input=layers.Input(shape=(latent_dim,),name='z_sampling')\n",
    "x=layers.Dense(width,activation=\"linear\")(decoder_input)\n",
    "for i in range(depth-1):\n",
    "    x=layers.Dense(width,activation=\"elu\")(x)\n",
    "output=layers.Dense(traindata.shape[1],activation=\"softplus\")(x) #can also use other activations that return strictly positive lambda parameter values\n",
    "decoder=Model(decoder_input,output,name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine for complete vae\n",
    "output_seq = decoder(encoder(input_seq)[2])\n",
    "vae = Model(input_seq, output_seq, name='vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define loss function\n",
    "#likelihood term is Poisson loss\n",
    "#clipped to avoid floating point errors in logarithm\n",
    "reconstruction_loss = output_seq - input_seq - input_seq * K.log(output_seq) + input_seq * K.log(K.clip(input_seq, 1e-07, data.max()))\n",
    "reconstruction_loss = K.sum(reconstruction_loss, axis=-1)\n",
    "#regularisation term is (twice?) the negative of the KL divergence of a diagonal multivariate normal and a standard normal\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var) \n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "#regularisation term is downweighed by weight parameter\n",
    "kl_loss *= -1/weight/weight/traindata.shape[1] \n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariloumercedes/miniconda3/envs/keras/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output decoder missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to decoder.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    }
   ],
   "source": [
    "vae.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks\n",
    "#save only weights for the best performing model\n",
    "#according to validation loss\n",
    "checkpointer=keras.callbacks.ModelCheckpoint(\n",
    "              filepath=out+\"_weights.hdf5\",\n",
    "              verbose=0,\n",
    "              save_best_only=True,\n",
    "              save_weights_only=True,\n",
    "              monitor=\"val_loss\",\n",
    "              period=1)\n",
    "\n",
    "#stop if no improvement for 'patience' number of updates\n",
    "earlystop=keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                        min_delta=0,\n",
    "                                        patience=patience)\n",
    "\n",
    "#half learning rate if no improvement for .25*patience number of updates\n",
    "reducelr=keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                           factor=0.5,\n",
    "                                           patience=int(patience/4),\n",
    "                                           verbose=1,\n",
    "                                           mode='auto',\n",
    "                                           min_delta=0,\n",
    "                                           cooldown=0,\n",
    "                                           min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_predictions=keras.callbacks.LambdaCallback(\n",
    "         on_epoch_end=lambda epoch,\n",
    "         logs:saveLDpos(encoder=encoder,\n",
    "                        predgen=data,\n",
    "                        samples=names,\n",
    "                        batch_size=batch_size,\n",
    "                        epoch=epoch,\n",
    "                        frequency=prediction_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 703 samples, validate on 79 samples\n",
      "Epoch 1/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 15956.0348 - val_loss: 3363.4790\n",
      "Epoch 2/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2972.7873 - val_loss: 3014.0432\n",
      "Epoch 3/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2776.8832 - val_loss: 2986.1022\n",
      "Epoch 4/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2754.0377 - val_loss: 2981.7470\n",
      "Epoch 5/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2753.7825 - val_loss: 3004.4153\n",
      "Epoch 6/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2754.7493 - val_loss: 2990.1944\n",
      "Epoch 7/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2746.1221 - val_loss: 2973.3034\n",
      "Epoch 8/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2742.0581 - val_loss: 2959.4346\n",
      "Epoch 9/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2742.8765 - val_loss: 2976.1448\n",
      "Epoch 10/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2741.0684 - val_loss: 2971.3249\n",
      "Epoch 11/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2739.1053 - val_loss: 2939.1939\n",
      "Epoch 12/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 2731.4250 - val_loss: 2930.1336\n",
      "Epoch 13/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 2726.5425 - val_loss: 2911.3575\n",
      "Epoch 14/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 2694.0925 - val_loss: 2835.8862\n",
      "Epoch 15/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2571.0999 - val_loss: 2791.7615\n",
      "Epoch 16/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 2513.1867 - val_loss: 2781.7888\n",
      "Epoch 17/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 2491.2890 - val_loss: 2756.2831\n",
      "Epoch 18/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 2467.5259 - val_loss: 2697.1282\n",
      "Epoch 19/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2491.2265 - val_loss: 2715.8320\n",
      "Epoch 20/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2457.3259 - val_loss: 2677.9481\n",
      "Epoch 21/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2383.2887 - val_loss: 2592.3912\n",
      "Epoch 22/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2341.1330 - val_loss: 2574.5223\n",
      "Epoch 23/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2328.1122 - val_loss: 2567.6504\n",
      "Epoch 24/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2305.0000 - val_loss: 2551.6827\n",
      "Epoch 25/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2301.2226 - val_loss: 2559.2281\n",
      "Epoch 26/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2290.5206 - val_loss: 2533.2662\n",
      "Epoch 27/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2280.7807 - val_loss: 2537.9124\n",
      "Epoch 28/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2276.9283 - val_loss: 2510.8573\n",
      "Epoch 29/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2254.6050 - val_loss: 2515.0193\n",
      "Epoch 30/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2242.0087 - val_loss: 2495.5025\n",
      "Epoch 31/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 2227.3406 - val_loss: 2486.2613\n",
      "Epoch 32/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 2221.8697 - val_loss: 2472.4717\n",
      "Epoch 33/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 2345.9147 - val_loss: 2731.1437\n",
      "Epoch 34/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 2460.6357 - val_loss: 2585.6921\n",
      "Epoch 35/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 2287.8466 - val_loss: 2541.4208\n",
      "Epoch 36/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 2270.9557 - val_loss: 2516.8627\n",
      "Epoch 37/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 2269.7824 - val_loss: 2565.1324\n",
      "Epoch 38/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 2327.9719 - val_loss: 2544.3601\n",
      "Epoch 39/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 2226.1959 - val_loss: 2482.5253\n",
      "Epoch 40/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2197.3363 - val_loss: 2497.6041\n",
      "Epoch 41/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 2181.8795 - val_loss: 2445.7613\n",
      "Epoch 42/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2178.0227 - val_loss: 2464.5202\n",
      "Epoch 43/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2247.3684 - val_loss: 2804.2273\n",
      "Epoch 44/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2670.3745 - val_loss: 2628.8699\n",
      "Epoch 45/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2391.9303 - val_loss: 2696.6698\n",
      "Epoch 46/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2303.9670 - val_loss: 2503.4273\n",
      "Epoch 47/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2234.5639 - val_loss: 2484.3806\n",
      "Epoch 48/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2206.3711 - val_loss: 2477.1022\n",
      "Epoch 49/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2189.1782 - val_loss: 2478.5196\n",
      "Epoch 50/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2181.0746 - val_loss: 2430.1017\n",
      "Epoch 51/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2160.6730 - val_loss: 2473.7385\n",
      "Epoch 52/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2153.3530 - val_loss: 2435.2477\n",
      "Epoch 53/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2147.9295 - val_loss: 2460.0147\n",
      "Epoch 54/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2143.4327 - val_loss: 2407.9018\n",
      "Epoch 55/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2121.7281 - val_loss: 2428.3290\n",
      "Epoch 56/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2109.6285 - val_loss: 2405.7851\n",
      "Epoch 57/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2099.3504 - val_loss: 2404.4242\n",
      "Epoch 58/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2094.0658 - val_loss: 2390.0904\n",
      "Epoch 59/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2092.4743 - val_loss: 2401.8518\n",
      "Epoch 60/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 2086.1910 - val_loss: 2371.6835\n",
      "Epoch 61/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2066.0709 - val_loss: 2375.2523\n",
      "Epoch 62/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2058.1593 - val_loss: 2375.1339\n",
      "Epoch 63/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2058.8929 - val_loss: 2355.0039\n",
      "Epoch 64/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2048.6739 - val_loss: 2359.6758\n",
      "Epoch 65/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2052.6444 - val_loss: 2397.5594\n",
      "Epoch 66/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2285.0025 - val_loss: 2668.2550\n",
      "Epoch 67/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2300.3839 - val_loss: 2504.7087\n",
      "Epoch 68/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2200.2031 - val_loss: 2457.0591\n",
      "Epoch 69/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2162.1233 - val_loss: 2430.7609\n",
      "Epoch 70/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2128.5557 - val_loss: 2423.5628\n",
      "Epoch 71/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2115.0936 - val_loss: 2428.3384\n",
      "Epoch 72/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2109.5100 - val_loss: 2398.0885\n",
      "Epoch 73/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2076.2152 - val_loss: 2383.4433\n",
      "Epoch 74/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2062.8579 - val_loss: 2384.6531\n",
      "Epoch 75/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2060.0102 - val_loss: 2381.6479\n",
      "Epoch 76/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2046.6618 - val_loss: 2394.6972\n",
      "Epoch 77/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2040.1859 - val_loss: 2381.0079\n",
      "Epoch 78/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2052.4407 - val_loss: 2392.1479\n",
      "Epoch 79/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2039.9441 - val_loss: 2364.5536\n",
      "Epoch 80/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2022.3124 - val_loss: 2354.1054\n",
      "Epoch 81/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2012.0278 - val_loss: 2347.4466\n",
      "Epoch 82/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2006.8102 - val_loss: 2342.7284\n",
      "Epoch 83/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2015.8864 - val_loss: 2363.6757\n",
      "Epoch 84/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2234.4127 - val_loss: 2466.5477\n",
      "Epoch 85/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2136.0491 - val_loss: 2377.0061\n",
      "Epoch 86/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2036.1735 - val_loss: 2328.6249\n",
      "Epoch 87/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2027.4796 - val_loss: 2375.2196\n",
      "Epoch 88/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2058.3504 - val_loss: 2352.8117\n",
      "Epoch 89/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2007.7370 - val_loss: 2317.4367\n",
      "Epoch 90/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1988.7190 - val_loss: 2323.1581\n",
      "Epoch 91/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2031.9608 - val_loss: 2509.4106\n",
      "Epoch 92/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2052.9008 - val_loss: 2342.5679\n",
      "Epoch 93/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1987.7625 - val_loss: 2350.2010\n",
      "Epoch 94/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1961.4122 - val_loss: 2344.7352\n",
      "Epoch 95/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 1955.7355 - val_loss: 2341.2926\n",
      "Epoch 96/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 1947.5740 - val_loss: 2331.0478\n",
      "Epoch 97/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1943.1807 - val_loss: 2349.9119\n",
      "Epoch 98/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1942.0939 - val_loss: 2359.2891\n",
      "Epoch 99/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1940.2646 - val_loss: 2367.0633\n",
      "Epoch 100/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1935.8710 - val_loss: 2355.0154\n",
      "Epoch 101/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 1943.2028 - val_loss: 2358.4012\n",
      "Epoch 102/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1996.7347 - val_loss: 2409.2951\n",
      "Epoch 103/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 2045.2858 - val_loss: 2349.3469\n",
      "Epoch 104/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 1974.9879 - val_loss: 2324.2564\n",
      "Epoch 105/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1942.2251 - val_loss: 2344.6134\n",
      "Epoch 106/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1930.5337 - val_loss: 2343.4022\n",
      "Epoch 107/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 1915.9876 - val_loss: 2354.5083\n",
      "Epoch 108/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1912.9720 - val_loss: 2360.2516\n",
      "Epoch 109/300\n",
      "703/703 [==============================] - 4s 6ms/step - loss: 1943.0218 - val_loss: 2364.2233\n",
      "Epoch 110/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1922.6307 - val_loss: 2346.9938\n",
      "Epoch 111/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1910.3826 - val_loss: 2356.3804\n",
      "Epoch 112/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1899.7545 - val_loss: 2357.8251\n",
      "Epoch 113/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1904.9276 - val_loss: 2350.9473\n",
      "Epoch 114/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1893.8196 - val_loss: 2359.6620\n",
      "\n",
      "Epoch 00114: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 115/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1867.1831 - val_loss: 2348.5662\n",
      "Epoch 116/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1851.2166 - val_loss: 2358.9289\n",
      "Epoch 117/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1847.2504 - val_loss: 2359.6064\n",
      "Epoch 118/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1842.7903 - val_loss: 2377.6998\n",
      "Epoch 119/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1840.3630 - val_loss: 2372.7777\n",
      "Epoch 120/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1837.1345 - val_loss: 2376.3292\n",
      "Epoch 121/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1834.7547 - val_loss: 2382.9282\n",
      "Epoch 122/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1831.3371 - val_loss: 2382.0148\n",
      "Epoch 123/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1829.0036 - val_loss: 2393.9847\n",
      "Epoch 124/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1829.9478 - val_loss: 2394.3117\n",
      "Epoch 125/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1826.5566 - val_loss: 2420.9161\n",
      "Epoch 126/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1827.5808 - val_loss: 2399.8937\n",
      "Epoch 127/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1822.1849 - val_loss: 2398.4213\n",
      "Epoch 128/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1818.5906 - val_loss: 2406.8051\n",
      "Epoch 129/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1818.6033 - val_loss: 2411.7409\n",
      "Epoch 130/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1816.7534 - val_loss: 2411.6222\n",
      "Epoch 131/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1812.0006 - val_loss: 2423.4108\n",
      "Epoch 132/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1810.9488 - val_loss: 2419.9161\n",
      "Epoch 133/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1809.7363 - val_loss: 2422.0250\n",
      "Epoch 134/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1807.6321 - val_loss: 2434.7128\n",
      "Epoch 135/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1805.3872 - val_loss: 2428.7351\n",
      "Epoch 136/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1804.8307 - val_loss: 2434.0847\n",
      "Epoch 137/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1803.3707 - val_loss: 2431.3069\n",
      "Epoch 138/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1804.6380 - val_loss: 2445.2278\n",
      "Epoch 139/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1809.5802 - val_loss: 2444.4252\n",
      "\n",
      "Epoch 00139: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 140/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1788.3126 - val_loss: 2430.3978\n",
      "Epoch 141/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1779.3936 - val_loss: 2432.3265\n",
      "Epoch 142/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1774.0781 - val_loss: 2442.4649\n",
      "Epoch 143/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1773.1743 - val_loss: 2453.2032\n",
      "Epoch 144/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1771.6830 - val_loss: 2449.3775\n",
      "Epoch 145/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1769.9260 - val_loss: 2459.3596\n",
      "Epoch 146/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1767.1389 - val_loss: 2462.2829\n",
      "Epoch 147/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1768.2396 - val_loss: 2466.0882\n",
      "Epoch 148/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1766.3850 - val_loss: 2470.6237\n",
      "Epoch 149/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1764.2472 - val_loss: 2473.0764\n",
      "Epoch 150/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1763.2194 - val_loss: 2476.0005\n",
      "Epoch 151/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1761.7697 - val_loss: 2483.0948\n",
      "Epoch 152/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1760.6476 - val_loss: 2483.9817\n",
      "Epoch 153/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1759.0356 - val_loss: 2488.3733\n",
      "Epoch 154/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1756.9265 - val_loss: 2485.8475\n",
      "Epoch 155/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1757.6378 - val_loss: 2492.4946\n",
      "Epoch 156/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1754.7057 - val_loss: 2496.3559\n",
      "Epoch 157/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1755.2965 - val_loss: 2494.7970\n",
      "Epoch 158/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1753.0904 - val_loss: 2500.8930\n",
      "Epoch 159/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1751.8378 - val_loss: 2502.4370\n",
      "Epoch 160/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1750.5062 - val_loss: 2513.1215\n",
      "Epoch 161/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1749.2509 - val_loss: 2511.3897\n",
      "Epoch 162/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1752.5175 - val_loss: 2531.0726\n",
      "Epoch 163/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1755.1589 - val_loss: 2521.6370\n",
      "Epoch 164/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1747.6437 - val_loss: 2517.1520\n",
      "\n",
      "Epoch 00164: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 165/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1735.9678 - val_loss: 2520.8931\n",
      "Epoch 166/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1732.4902 - val_loss: 2517.7853\n",
      "Epoch 167/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1731.7364 - val_loss: 2525.2234\n",
      "Epoch 168/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1730.4404 - val_loss: 2530.2331\n",
      "Epoch 169/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1729.8184 - val_loss: 2536.3691\n",
      "Epoch 170/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1728.9098 - val_loss: 2533.7720\n",
      "Epoch 171/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1727.8343 - val_loss: 2535.9414\n",
      "Epoch 172/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1727.4564 - val_loss: 2537.7713\n",
      "Epoch 173/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1726.1104 - val_loss: 2541.3475\n",
      "Epoch 174/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1725.7028 - val_loss: 2546.1181\n",
      "Epoch 175/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1725.9162 - val_loss: 2549.2220\n",
      "Epoch 176/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1723.8953 - val_loss: 2550.5340\n",
      "Epoch 177/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1723.0198 - val_loss: 2551.1844\n",
      "Epoch 178/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1721.9888 - val_loss: 2553.8800\n",
      "Epoch 179/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1722.2177 - val_loss: 2562.6326\n",
      "Epoch 180/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1721.0122 - val_loss: 2562.4395\n",
      "Epoch 181/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1720.6768 - val_loss: 2560.2571\n",
      "Epoch 182/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1719.9003 - val_loss: 2561.3981\n",
      "Epoch 183/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1719.3479 - val_loss: 2566.3070\n",
      "Epoch 184/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1718.0331 - val_loss: 2571.4491\n",
      "Epoch 185/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1716.8176 - val_loss: 2569.7805\n",
      "Epoch 186/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1717.0575 - val_loss: 2569.8043\n",
      "Epoch 187/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1716.4700 - val_loss: 2579.1209\n",
      "Epoch 188/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1715.7733 - val_loss: 2578.9475\n",
      "Epoch 189/300\n",
      "703/703 [==============================] - 4s 5ms/step - loss: 1713.9528 - val_loss: 2580.7055\n",
      "\n",
      "Epoch 00189: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "VAE run time: 725.0083630084991 seconds\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "t1=time.time()\n",
    "history=vae.fit(x=traindata, \n",
    "               y=None,\n",
    "               shuffle=True,\n",
    "               epochs=300, \n",
    "               callbacks=[checkpointer,earlystop,reducelr,print_predictions],\n",
    "               validation_data=(testdata,None), \n",
    "               batch_size=32)\n",
    "t2=time.time()\n",
    "vaetime=t2-t1\n",
    "print(\"VAE run time: \"+str(vaetime)+\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save training history\n",
    "h=pd.DataFrame(history.history)\n",
    "h.to_csv(out+\"_history.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project to latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict latent space coords for all samples from weights minimizing val loss\n",
    "vae.load_weights(out+\"_weights.hdf5\")\n",
    "pred=encoder.predict(data,batch_size=batch_size) \n",
    "p=pd.DataFrame()\n",
    "\n",
    "p['mean1']=pred[0][:,0]\n",
    "p['mean2']=pred[0][:,1]\n",
    "p['mean3']=pred[0][:,2]\n",
    "p['sd1']=pred[1][:,0]\n",
    "p['sd2']=pred[1][:,1]\n",
    "p['sd3']=pred[1][:,2]\n",
    "pred=p\n",
    "\n",
    "pred['sampleID']=names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.loc[pred.sampleID.isin(testsamples), \"status\"] = \"validation\"\n",
    "pred.loc[pred.status.isnull(), \"status\"] = \"training\"\n",
    "meta_dict = dict(zip(meta.s_Sample, meta.species))\n",
    "pred['species'] = pred.sampleID.map(meta_dict)\n",
    "pred.to_csv(out+'latent_coords.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
